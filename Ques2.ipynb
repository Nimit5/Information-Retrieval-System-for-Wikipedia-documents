{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc101e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nimit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import csv \n",
    "import string\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root folder where the cleaned text files to be located\n",
    "corpus_root = '/home/nimit/IITK/IR_CS657A/Assign1/Solution/english-corpora-processed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the list of files\n",
    "filelists = PlaintextCorpusReader(corpus_root, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List down the IDs of the files read from the local storage\n",
    "doc_list=filelists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size=len(filelists.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225dff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_normalised_length=np.zeros(corpus_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2997542",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len_list=[]\n",
    "global_matrix=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7dc4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "\n",
    "for filename in filelists.fileids():\n",
    "    index = defaultdict(int)\n",
    "    with open(corpus_root+'/'+str(filename),'r+')as f:\n",
    "        text=f.read()\n",
    "        tokens_words=nltk.word_tokenize(text)\n",
    "        for word in tokens_words[1:-1]: \n",
    "            index[word]+=1\n",
    "        \n",
    "    doc_len_list.append(len(index))\n",
    "    global_matrix.append(index)\n",
    "    for item in index:\n",
    "        doc_normalised_length[i]+=((1+np.log(index[item]))**2)\n",
    "        try:\n",
    "            unique_words_dict[item][(str(filename).replace('.txt',''))]=index[item]\n",
    "        except:\n",
    "            unique_words_dict[item]={}\n",
    "            unique_words_dict[item][(str(filename).replace('.txt',''))]=index[item]\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nimit/IITK/IR_CS657A/Assign1/Solution/posting_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_words_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d43379",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nimit/IITK/IR_CS657A/Assign1/Solution/doc_len_list.pkl', 'wb') as f:\n",
    "    pickle.dump(doc_len_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nimit/IITK/IR_CS657A/Assign1/Solution/global_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(global_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nimit/IITK/IR_CS657A/Assign1/Solution/doc_normalised_length.pkl', 'wb') as f:\n",
    "    pickle.dump(doc_normalised_length, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nimit/IITK/IR_CS657A/Assign1/Solution/posting_dict.pkl', 'rb') as f:\n",
    "    loaded_posting_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nimit/IITK/IR_CS657A/Assign1/Solution/doc_len_list.pkl', 'rb') as f:\n",
    "    loaded_doc_len_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5378cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nimit/IITK/IR_CS657A/Assign1/Solution/global_matrix.pkl', 'rb') as f:\n",
    "    loaded_global_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b91f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nimit/IITK/IR_CS657A/Assign1/Solution/doc_normalised_length.pkl', 'rb') as f:\n",
    "    loaded_doc_normalised_length=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889064f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_all = len(loaded_posting_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe40e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query(query):\n",
    "    \n",
    "    boolean_result = []\n",
    "    binary_array_per_word=[]\n",
    "    binary_array_all_words=[]\n",
    "    \n",
    "    for term in query:\n",
    "        if term in loaded_posting_dict:\n",
    "            binary_array_per_word=[0]*corpus_size\n",
    "            temp_dict=loaded_posting_dict[term]\n",
    "            \n",
    "            for item in temp_dict:\n",
    "                temp2=doc_list.index((str(item)+\".txt\"))\n",
    "                binary_array_per_word[temp2]=1\n",
    "            \n",
    "            binary_array_all_words.append(binary_array_per_word)\n",
    "        else:\n",
    "            print(word,\" not found\")\n",
    "    \n",
    "    for j in range(len(query)-1):\n",
    "        word1=binary_array_all_words[0]\n",
    "        word2=binary_array_all_words[1]\n",
    "\n",
    "        bitwise_op = [w1 & w2 for (w1,w2) in zip(word1,word2)]\n",
    "            \n",
    "        binary_array_all_words.remove(word1)\n",
    "        binary_array_all_words.remove(word2)\n",
    "        binary_array_all_words.insert(0,bitwise_op)\n",
    "  \n",
    "    res = binary_array_all_words[0]\n",
    "    cnt = 0\n",
    "    \n",
    "    for item in res:\n",
    "        if item==1:\n",
    "            boolean_result.append(doc_list[cnt])\n",
    "        cnt = cnt+1\n",
    "    \n",
    "    return boolean_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629976fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_score(query):\n",
    "    len_query=len(query)\n",
    "    tf_query={}\n",
    "    arr=np.zeros((len_query+1,corpus_size))\n",
    "    for i in range(len_query):\n",
    "        try:\n",
    "            tf_query[query[i]]+=1\n",
    "        except:\n",
    "            tf_query[query[i]]=1\n",
    "            \n",
    "        temp20 = loaded_posting_dict[query[i]]\n",
    "        \n",
    "        for item in temp20:\n",
    "            temp=item\n",
    "            temp2=doc_list.index((str(temp)+\".txt\"))\n",
    "            #arr[i][temp2]=1+np.log(temp20[item])\n",
    "            arr[i][temp2]=(temp20[item])\n",
    "            arr[-1][temp2]+=arr[i][temp2]**2\n",
    "    \n",
    "    \n",
    "    # Processing Query\n",
    "    query_square=0\n",
    "    for item in tf_query:\n",
    "        #tf_query[item]=1+np.log(tf_query[item])\n",
    "        tf_query[item]=tf_query[item]*(np.log(corpus_size/len(loaded_posting_dict[item]))) #need to handle this case when item is not in dict\n",
    "        query_square+=tf_query[item]**2\n",
    "    \n",
    "    #query_root=math.sqrt(query_square)\n",
    "    q_arr=np.zeros(len_query)\n",
    "    \n",
    "    y=0\n",
    "    for z in tf_query:\n",
    "        q_arr[y]=tf_query[z]\n",
    "        y+=1\n",
    "        #/query_root\n",
    "    \n",
    "    # Final similarity check\n",
    "    \n",
    "    doc_score={}\n",
    "    for i in range(corpus_size):\n",
    "        s=0\n",
    "        for j in range(len_query):\n",
    "            s+=(arr[j][i]*q_arr[j])\n",
    "        doc_score[doc_list[i]]=s\n",
    "    doc_score = {k: v for k, v in sorted(doc_score.items(), key=lambda x: x[1],reverse=True)}\n",
    "    return doc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662cfcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(query,k1=1.2,b=0.75):\n",
    "    \n",
    "    bm25_scores = {}\n",
    "    #calculating avg_doc_len value\n",
    "    avg_doc_len_ = sum(loaded_doc_len_list) / corpus_size\n",
    "    \n",
    "    for index in range(corpus_size):\n",
    "        scr = 0.0\n",
    "        doc_len = loaded_doc_len_list[index]\n",
    "        term_frequency = loaded_global_matrix[index]\n",
    "\n",
    "        for term in query:\n",
    "            if term not in term_frequency:\n",
    "                continue\n",
    "            \n",
    "            count=len(loaded_posting_dict[term])\n",
    "            freq = term_frequency[term]\n",
    "            idf_val=np.log(1 + (corpus_size - count + 0.5) / (count + 0.5))\n",
    "            num = idf_val * freq * (k1 + 1)\n",
    "            deno = freq + k1 * (1 - b + b * doc_len / avg_doc_len_)\n",
    "            scr += (num/deno)\n",
    "        \n",
    "        bm25_scores[doc_list[index]]=scr\n",
    "    \n",
    "    final_ans=dict(sorted(bm25_scores.items(), key=lambda item: item[1],reverse=True))\n",
    "    return final_ans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
